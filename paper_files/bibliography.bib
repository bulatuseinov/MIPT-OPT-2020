@ARTICLE{adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@ARTICLE{barzilaiborwein,
    title={Barzilai-Borwein Step Size for Stochastic Gradient Descent},
    author={Conghui Tan and Shiqian Ma and Yu-Hong Dai and Yuqiu Qian},
    year={2016},
    eprint={1605.04131},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@BOOK{onlineBFGS,
  title = 	 {A Stochastic Quasi-Newton Method for Online Convex Optimization},
  author = 	 {Nicol N. Schraudolph and Jin Yu and Simon Günter},
  booktitle = 	 {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  pages = 	 {436--443},
  year = 	 {2007},
  editor = 	 {Marina Meila and Xiaotong Shen},
  volume = 	 {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Juan, Puerto Rico},
  month = 	 {21--24 Mar},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v2/schraudolph07a/schraudolph07a.pdf},
  url = 	 {http://proceedings.mlr.press/v2/schraudolph07a.html},
  abstract = 	 {We develop stochastic variants of the well-known BFGS quasi-Newton optimization method, in both full and memory-limited (LBFGS) forms, for online optimization of convex functions. The resulting algorithm performs comparably to a well-tuned natural gradient descent but is scalable to very high-dimensional problems. On standard benchmarks in natural language processing, it asymptotically outperforms previous stochastic gradient methods for parameter estimation in conditional random fields. We are working on analyzing the convergence of online (L)BFGS, and extending it to nonconvex optimization problems.}
}

@ARTICLE{sampledbfgs,
    title={Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample},
    author={Albert S. Berahas and Majid Jahani and Martin Takáč},
    year={2019},
    eprint={1901.09997},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@BOOK{numopt,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@ARTICLE{multibatchLBFGS,
    title={A Multi-Batch L-BFGS Method for Machine Learning},
    author={Albert S. Berahas and Jorge Nocedal and Martin Takáč},
    year={2016},
    eprint={1605.06049},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@ARTICLE{BB-DL,
  title={Barzilai--Borwein-based adaptive learning rate for deep learning},
  author={Liang, Jinxiu and Xu, Yong and Bao, Chenglong and Quan, Yuhui and Ji, Hui},
  journal={Pattern Recognition Letters},
  volume={128},
  pages={197--203},
  year={2019},
  publisher={Elsevier}
}

@ARTICLE{MNIST,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@TECHREPORT{CIFAR10,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@misc{SGDconvergence,
    title={SGD: General Analysis and Improved Rates},
    author={Robert Mansel Gower and Nicolas Loizou and Xun Qian and Alibek Sailanbayev and Egor Shulgin and Peter Richtarik},
    year={2019},
    eprint={1901.09401},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{HBconvergence,
    title={Global convergence of the Heavy-ball method for convex optimization},
    author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
    year={2014},
    eprint={1412.7457},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@misc{SaddlePoints,
    title={Escaping Saddle Points with Adaptive Gradient Methods},
    author={Matthew Staib and Sashank J. Reddi and Satyen Kale and Sanjiv Kumar and Suvrit Sra},
    year={2019},
    eprint={1901.09149},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{RMSprop,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@misc{BBconvergence,
    title={Stabilized Barzilai-Borwein method},
    author={Oleg Burdakov and Yu-Hong Dai and Na Huang},
    year={2019},
    eprint={1907.06409},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@ARTICLE{vgg,
    title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
    author={Karen Simonyan and Andrew Zisserman},
    year={2014},
    eprint={1409.1556},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{LBFGSunstable,
    title={On the Acceleration of L-BFGS with Second-Order Information and Stochastic Batches},
    author={Jie Liu and Yu Rong and Martin Takac and Junzhou Huang},
    year={2018},
    eprint={1807.05328},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{Polyak,
  title={Введение в оптимизацию},
  author={Polyak, B. T.},
  year={1983},
  publisher={Наука. Гл. ред. физ.-мат. лит.},
  language="russian"
}

@article{GDoverview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@article{NAG,
author={NESTEROV, Y. E.},
title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
journal={Dokl. Akad. Nauk SSSR},
ISSN="",
publisher="",
year="1983",
month="",
volume="269",
number="",
pages="543-547",
URL="https://ci.nii.ac.jp/naid/10029946121/en/",
DOI="",
}

@article{AdaGrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@book{BFGSorig,
  title={Practical methods of optimization},
  author={Fletcher, R.},
  number={т. 1},
  isbn={9780471915478},
  lccn={87008126},
  series={Wiley-Interscience publication},
  url={https://books.google.ru/books?id=3EzvAAAAMAAJ},
  year={1987},
  publisher={Wiley}
}

@article{BBorig,
    author = {BARZILAI, JONATHAN and BORWEIN, JONATHAN M.},
    title = "{Two-Point Step Size Gradient Methods}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {8},
    number = {1},
    pages = {141-148},
    year = {1988},
    month = {01},
    abstract = "{We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and gradient, these algorithms achieve better performance and cheaper computation than the classical steepest-descent method. We indicate a convergence analysis of the method in the two-dimensional quadratic case. The behaviour is highly remarkable and the analysis entirely nonstandard.}",
    issn = {0272-4979},
    doi = {10.1093/imanum/8.1.141},
    url = {https://doi.org/10.1093/imanum/8.1.141}
}


@article{SGDorig,
author = "Kiefer, J. and Wolfowitz, J.",
doi = "10.1214/aoms/1177729392",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "462--466",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Estimation of the Maximum of a Regression Function",
url = "https://doi.org/10.1214/aoms/1177729392",
volume = "23",
year = "1952"
}

@article{SGDconvergenceNonconvex,
    title={Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions},
    author={Yunwen Lei and Ting Hu and Guiying Li and Ke Tang},
    year={2019},
    eprint={1902.00908},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{NAGstochastic,
    title={On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
    author={Mahmoud Assran and Michael Rabbat},
    year={2020},
    eprint={2002.12414},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{DFPorig,
  title={Variable metric method for minimization},
  author={Davidon, William C},
  journal={SIAM Journal on Optimization},
  volume={1},
  number={1},
  pages={1--17},
  year={1991},
  publisher={SIAM}
}

@article{MNISTexperiments__4pages,
  title={Assessing four neural networks on handwritten digit recognition dataset (MNIST)},
  author={Chen, Feiyang and Chen, Nan and Mao, Hanyang and Hu, Hanlin},
  journal={arXiv preprint arXiv:1811.08278},
  year={2018}
}

@article{CIFAR10experiments,
  title={CIFAR10 to compare visual recognition performance between deep neural networks and humans},
  author={Ho-Phuoc, Tien},
  journal={arXiv preprint arXiv:1811.07270},
  year={2018}
}

@article{MNISTexperiments,
author = {Mishra, Shashank and Kamaraj, Malathi and Senthilkumar, K},
year = {2018},
month = {05},
pages = {},
title = {DIGIT RECOGNITION USING DEEP LEARNING}
}