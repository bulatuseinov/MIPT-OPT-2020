{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import cifar10 # to load dataset\n",
    "\n",
    "from utils import compute_stats, get_grad\n",
    "from MB_LBFGS import LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10 * (50000 // 256)                      # note each iteration is NOT an epoch\n",
    "ghost_batch = 128\n",
    "overlap_ratio = 0.25                # should be in (0, 0.5)\n",
    "lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (10000, 3, 32, 32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "    \n",
    "#%% Create neural network model\n",
    "        \n",
    "if(cuda):\n",
    "    torch.cuda.manual_seed(2018)\n",
    "    model = ConvNet().cuda() \n",
    "else:\n",
    "    torch.manual_seed(2018)\n",
    "    model = ConvNet()\n",
    "    \n",
    "#%% Define helper functions\n",
    "\n",
    "# Forward pass\n",
    "if(cuda):\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
    "else:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
    "\n",
    "# Forward pass through the network given the input\n",
    "if(cuda):\n",
    "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
    "else:\n",
    "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
    "\n",
    "# Do the forward pass, then compute the accuracy\n",
    "accfun   = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Powell damping...\n",
      "Iter: 0 lr: 1.0 Training Loss: 2.3005438346052167 Test Loss: 2.300460264635086 Test Accuracy: 9.99\n",
      "Applying Powell damping...\n",
      "Iter: 195 lr: 1.0 Training Loss: 1.8164344183778762 Test Loss: 1.8156640972018245 Test Accuracy: 34.2\n",
      "Iter: 390 lr: 1.0 Training Loss: 1.6860568000459666 Test Loss: 1.6852852741122248 Test Accuracy: 39.780000000000015\n",
      "Iter: 585 lr: 1.0 Training Loss: 1.6034406588268284 Test Loss: 1.606543820750714 Test Accuracy: 42.47000000000003\n",
      "Applying Powell damping...\n",
      "Iter: 780 lr: 1.0 Training Loss: 1.5694034939670554 Test Loss: 1.5759196725606912 Test Accuracy: 43.44\n",
      "Iter: 975 lr: 1.0 Training Loss: 1.5486393788290023 Test Loss: 1.5583452074170112 Test Accuracy: 43.98\n",
      "Iter: 1170 lr: 1.0 Training Loss: 1.5222140706181528 Test Loss: 1.5313565752744667 Test Accuracy: 45.02000000000001\n",
      "Iter: 1365 lr: 1.0 Training Loss: 1.504505547719001 Test Loss: 1.516534092879296 Test Accuracy: 45.280000000000015\n",
      "Iter: 1560 lr: 1.0 Training Loss: 1.4975804122304914 Test Loss: 1.512227636456489 Test Accuracy: 45.019999999999996\n",
      "Iter: 1755 lr: 1.0 Training Loss: 1.4808351711702346 Test Loss: 1.498644383311272 Test Accuracy: 46.139999999999986\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, line_search='None', debug=True)\n",
    "\n",
    "#%% Main training loop\n",
    "\n",
    "Ok_size = int(overlap_ratio*batch_size)\n",
    "Nk_size = int((1 - 2*overlap_ratio)*batch_size)\n",
    "\n",
    "# sample previous overlap gradient\n",
    "random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "Ok_prev = random_index[0:Ok_size]\n",
    "g_Ok_prev, obj_Ok_prev = get_grad(optimizer, X_train[Ok_prev], y_train[Ok_prev], opfun)\n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    \n",
    "    # sample current non-overlap and next overlap gradient\n",
    "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "    Ok = random_index[0:Ok_size]\n",
    "    Nk = random_index[Ok_size:(Ok_size + Nk_size)]\n",
    "    \n",
    "    # compute overlap gradient and objective\n",
    "    g_Ok, obj_Ok = get_grad(optimizer, X_train[Ok], y_train[Ok], opfun)\n",
    "    \n",
    "    # compute non-overlap gradient and objective\n",
    "    g_Nk, obj_Nk = get_grad(optimizer, X_train[Nk], y_train[Nk], opfun)\n",
    "    \n",
    "    # compute accumulated gradient over sample\n",
    "    g_Sk = overlap_ratio*(g_Ok_prev + g_Ok) + (1 - 2*overlap_ratio)*g_Nk\n",
    "        \n",
    "    # two-loop recursion to compute search direction\n",
    "    p = optimizer.two_loop_recursion(-g_Sk)\n",
    "                \n",
    "    # perform line search step\n",
    "    lr = optimizer.step(p, g_Ok, g_Sk=g_Sk)\n",
    "    \n",
    "    # compute previous overlap gradient for next sample\n",
    "    Ok_prev = Ok\n",
    "    g_Ok_prev, obj_Ok_prev = get_grad(optimizer, X_train[Ok_prev], y_train[Ok_prev], opfun)\n",
    "    \n",
    "    # curvature update\n",
    "    optimizer.curvature_update(g_Ok_prev, eps=0.2, damping=True)\n",
    "    \n",
    "    # compute statistics\n",
    "    if n_iter % int(50000 / 256) == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, \n",
    "                                                            y_test, opfun, accfun, ghost_batch=128)\n",
    "            print('Iter:',n_iter, 'lr:', lr, 'Training Loss:', train_loss, \n",
    "                  'Test Loss:', test_loss, 'Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "    \n",
    "#%% Create neural network model\n",
    "        \n",
    "if(cuda):\n",
    "    torch.cuda.manual_seed(2018)\n",
    "    model = ConvNet().cuda() \n",
    "else:\n",
    "    torch.manual_seed(2018)\n",
    "    model = ConvNet()\n",
    "    \n",
    "#%% Define helper functions\n",
    "\n",
    "# Forward pass\n",
    "if(cuda):\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
    "else:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
    "\n",
    "# Forward pass through the network given the input\n",
    "if(cuda):\n",
    "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
    "else:\n",
    "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
    "\n",
    "# Do the forward pass, then compute the accuracy\n",
    "accfun   = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 lr: 1.0 Training Loss: 2.3017808129262916 Test Loss: 2.3018424204111105 Test Accuracy: 10.679999999999998\n",
      "Iter: 195 lr: 1.0 Training Loss: 1.556580683267116 Test Loss: 1.5562827260375027 Test Accuracy: 43.57999999999999\n",
      "Iter: 390 lr: 1.0 Training Loss: 1.414362938771247 Test Loss: 1.427319056892395 Test Accuracy: 48.050000000000004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d20162e78785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# sample current non-overlap and next overlap gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrandom_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.permutation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmay_share_memory\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    \n",
    "    # sample current non-overlap and next overlap gradient\n",
    "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "    idx = random_index[0:batch_size]\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    X_batch = torch.from_numpy(X_train[idx]).cuda()\n",
    "    y_batch = torch.from_numpy(y_train[idx]).cuda().long().squeeze()\n",
    "    \n",
    "    loss = F.cross_entropy(model(X_batch), y_batch)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute statistics\n",
    "    if n_iter % int(50000 / 256) == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, \n",
    "                                                            y_test, opfun, accfun, ghost_batch=128)\n",
    "            print('Iter:',n_iter, 'lr:', lr, 'Training Loss:', train_loss, \n",
    "                  'Test Loss:', test_loss, 'Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
