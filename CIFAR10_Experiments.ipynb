{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import cifar10 # to load dataset\n",
    "\n",
    "from MultiBatchLBFGS.functions.utils import compute_stats, get_grad\n",
    "from MultiBatchLBFGS.functions.LBFGS import LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10 * (50000 // 256)                      # note each iteration is NOT an epoch\n",
    "ghost_batch = 128\n",
    "overlap_ratio = 0.25                # should be in (0, 0.5)\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3, 32, 32), (10000, 3, 32, 32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "    \n",
    "#%% Create neural network model\n",
    "        \n",
    "if(cuda):\n",
    "    torch.cuda.manual_seed(2018)\n",
    "    model = ConvNet().cuda() \n",
    "else:\n",
    "    torch.manual_seed(2018)\n",
    "    model = ConvNet()\n",
    "    \n",
    "#%% Define helper functions\n",
    "\n",
    "# Forward pass\n",
    "if(cuda):\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
    "else:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
    "\n",
    "# Forward pass through the network given the input\n",
    "if(cuda):\n",
    "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
    "else:\n",
    "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
    "\n",
    "# Do the forward pass, then compute the accuracy\n",
    "accfun   = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 lr: 1 Training Loss: 24.168714043273923 Test Loss: 24.37661487369537 Test Accuracy: 10.030000000000003\n",
      "Applying Powell damping...\n",
      "Applying Powell damping...\n",
      "Iter: 195 lr: 1 Training Loss: 1.7256874379014975 Test Loss: 1.7173157900214193 Test Accuracy: 38.22\n",
      "Iter: 390 lr: 1 Training Loss: 1.6697033354449273 Test Loss: 1.6632348521947862 Test Accuracy: 40.15\n",
      "Iter: 585 lr: 1 Training Loss: 1.6304542130970954 Test Loss: 1.62609387910366 Test Accuracy: 41.580000000000005\n",
      "Iter: 780 lr: 1 Training Loss: 1.6346229580450058 Test Loss: 1.6319526261806485 Test Accuracy: 39.97\n",
      "Iter: 975 lr: 1 Training Loss: 1.5859598353528976 Test Loss: 1.583344218420982 Test Accuracy: 43.25000000000002\n",
      "Iter: 1170 lr: 1 Training Loss: 1.5770777635312077 Test Loss: 1.5761734462618824 Test Accuracy: 43.64\n",
      "Iter: 1365 lr: 1 Training Loss: 1.5566475127029413 Test Loss: 1.55786863874197 Test Accuracy: 43.9\n",
      "Iter: 1560 lr: 1 Training Loss: 1.547496070587635 Test Loss: 1.5469452185869228 Test Accuracy: 44.09\n",
      "Iter: 1755 lr: 1 Training Loss: 1.5436069273829454 Test Loss: 1.5422350466132166 Test Accuracy: 43.989999999999995\n",
      "Applying Powell damping...\n",
      "Applying Powell damping...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, line_search='None', debug=True)\n",
    "\n",
    "#%% Main training loop\n",
    "\n",
    "Ok_size = int(overlap_ratio*batch_size)\n",
    "Nk_size = int((1 - 2*overlap_ratio)*batch_size)\n",
    "\n",
    "# sample previous overlap gradient\n",
    "random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "Ok_prev = random_index[0:Ok_size]\n",
    "g_Ok_prev, obj_Ok_prev = get_grad(optimizer, X_train[Ok_prev], y_train[Ok_prev], opfun)\n",
    "\n",
    "for n_iter in range(max_iter):\n",
    "    \n",
    "    # sample current non-overlap and next overlap gradient\n",
    "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "    Ok = random_index[0:Ok_size]\n",
    "    Nk = random_index[Ok_size:(Ok_size + Nk_size)]\n",
    "    \n",
    "    # compute overlap gradient and objective\n",
    "    g_Ok, obj_Ok = get_grad(optimizer, X_train[Ok], y_train[Ok], opfun)\n",
    "    \n",
    "    # compute non-overlap gradient and objective\n",
    "    g_Nk, obj_Nk = get_grad(optimizer, X_train[Nk], y_train[Nk], opfun)\n",
    "    \n",
    "    # compute accumulated gradient over sample\n",
    "    g_Sk = overlap_ratio*(g_Ok_prev + g_Ok) + (1 - 2*overlap_ratio)*g_Nk\n",
    "        \n",
    "    # two-loop recursion to compute search direction\n",
    "    p = optimizer.two_loop_recursion(-g_Sk)\n",
    "                \n",
    "    # perform line search step\n",
    "    lr = optimizer.step(p, g_Ok, g_Sk=g_Sk)\n",
    "    \n",
    "    # compute previous overlap gradient for next sample\n",
    "    Ok_prev = Ok\n",
    "    g_Ok_prev, obj_Ok_prev = get_grad(optimizer, X_train[Ok_prev], y_train[Ok_prev], opfun)\n",
    "    \n",
    "    # curvature update\n",
    "    optimizer.curvature_update(g_Ok_prev, eps=0.2, damping=True)\n",
    "    \n",
    "    # compute statistics\n",
    "    if n_iter % int(50000 / 256) == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, \n",
    "                                                            y_test, opfun, accfun, ghost_batch=128)\n",
    "            print('Iter:',n_iter, 'lr:', lr, 'Training Loss:', train_loss, \n",
    "                  'Test Loss:', test_loss, 'Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "    \n",
    "#%% Create neural network model\n",
    "        \n",
    "if(cuda):\n",
    "    torch.cuda.manual_seed(2018)\n",
    "    model = ConvNet().cuda() \n",
    "else:\n",
    "    torch.manual_seed(2018)\n",
    "    model = ConvNet()\n",
    "    \n",
    "#%% Define helper functions\n",
    "\n",
    "# Forward pass\n",
    "if(cuda):\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n",
    "else:\n",
    "    opfun = lambda X: model.forward(torch.from_numpy(X))\n",
    "\n",
    "# Forward pass through the network given the input\n",
    "if(cuda):\n",
    "    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n",
    "else:\n",
    "    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n",
    "\n",
    "# Do the forward pass, then compute the accuracy\n",
    "accfun   = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 lr: 1 Training Loss: 2.302923829541207 Test Loss: 2.302945973181725 Test Accuracy: 10.080000000000004\n",
      "Iter: 26 lr: 1 Training Loss: 2.144063091225624 Test Loss: 2.1451572744131098 Test Accuracy: 23.49000000000001\n",
      "Iter: 51 lr: 1 Training Loss: 1.8938649498200415 Test Loss: 1.8941563960075374 Test Accuracy: 31.069999999999993\n",
      "Iter: 76 lr: 1 Training Loss: 1.7951981131434445 Test Loss: 1.793898467612267 Test Accuracy: 32.60999999999999\n",
      "Iter: 101 lr: 1 Training Loss: 1.733583551299572 Test Loss: 1.7326497027158732 Test Accuracy: 37.29\n",
      "Iter: 126 lr: 1 Training Loss: 1.7204608139228816 Test Loss: 1.7207010856389995 Test Accuracy: 37.10999999999999\n",
      "Iter: 151 lr: 1 Training Loss: 1.6339990898704535 Test Loss: 1.6326930150508878 Test Accuracy: 40.64000000000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dcd84579f224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, \n\u001b[0;32m---> 23\u001b[0;31m                                                             y_test, opfun, accfun, ghost_batch=128)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# print data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Study/Term_6/OptMeth/Project/code/MultiBatchLBFGS/functions/utils.py\u001b[0m in \u001b[0;36mcompute_stats\u001b[0;34m(X_train, y_train, X_test, y_test, opfun, accfun, ghost_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# define training set ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrainops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmpl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# accumulate weighted training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b69015e541c8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mopfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mopfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for n_iter in range(3000):\n",
    "    \n",
    "    # sample current non-overlap and next overlap gradient\n",
    "    random_index = np.random.permutation(range(X_train.shape[0]))\n",
    "    idx = random_index[0:batch_size]\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    X_batch = torch.from_numpy(X_train[idx]).cuda()\n",
    "    y_batch = torch.from_numpy(y_train[idx]).cuda().long().squeeze()\n",
    "    \n",
    "    loss = F.cross_entropy(model(X_batch), y_batch)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute statistics\n",
    "    if n_iter % 25 == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, \n",
    "                                                            y_test, opfun, accfun, ghost_batch=128)\n",
    "\n",
    "            # print data\n",
    "            print('Iter:',n_iter+1, 'lr:', lr, 'Training Loss:', train_loss, \n",
    "                  'Test Loss:', test_loss, 'Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
